#!/usr/bin/env python3
"""
Export Temperature Prediction Model to C Header for Contiki-NG
This script converts the trained Random Forest model to embedded C code
using emlearn library, similar to occupancy_model.h format.
"""

import os
import sys
import joblib
import numpy as np

# Get the directory where this script is located
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))

# Try to import emlearn
try:
    import emlearn
    print(f"emlearn version: {emlearn.__version__}")
except ImportError:
    print("ERROR: emlearn not installed!")
    print("Install with: pip install emlearn")
    sys.exit(1)

def export_rf_model_to_c():
    """Export Random Forest model to C header file"""

    print("\n" + "="*70)
    print("EXPORTING TEMPERATURE RF MODEL TO C HEADER FILE")
    print("="*70)

    # Load the trained model and scaler
    model_path = os.path.join(SCRIPT_DIR, 'temperature_rf_model.joblib')
    scaler_path = os.path.join(SCRIPT_DIR, 'temperature_scaler.joblib')

    if not os.path.exists(model_path):
        print(f"ERROR: Model file not found: {model_path}")
        print("Please run 2023_indoor_air_quality_dataset_germany.py first to train the model")
        sys.exit(1)

    if not os.path.exists(scaler_path):
        print(f"ERROR: Scaler file not found: {scaler_path}")
        sys.exit(1)

    print("\n[1/4] Loading trained model...")
    rf_model = joblib.load(model_path)
    scaler = joblib.load(scaler_path)

    print(f"   Model loaded: {rf_model.n_estimators} trees, max_depth={rf_model.max_depth}")
    print(f"   Scaler loaded: min={scaler.data_min_[0]:.2f}, max={scaler.data_max_[0]:.2f}")

    # Convert model to C using emlearn
    print("\n[2/4] Converting Random Forest to C code using emlearn...")

    try:
        # Convert the RandomForest model and save to temporary file
        converter = emlearn.convert(rf_model, method='inline')

        # Save to temporary C file to extract the code
        temp_c_file = os.path.join(SCRIPT_DIR, 'temp_temperature_model.h')
        converter.save(file=temp_c_file, name='temperature_model')

        # Read the generated C code
        with open(temp_c_file, 'r') as f:
            c_code_raw = f.read()

        # Clean up temp file
        os.remove(temp_c_file)

        # Extract just the model code, removing header guards and includes
        # emlearn might add its own header guards
        c_code_lines = []
        for line in c_code_raw.split('\n'):
            # Skip header guards and common includes (we'll add our own)
            if any(x in line for x in ['#ifndef', '#define', '#endif', '#include <stdint.h>', '#include <math.h>']):
                if line.strip().startswith('#endif'):
                    continue  # Skip #endif
                elif line.strip().startswith('#ifndef') or line.strip().startswith('#define'):
                    continue  # Skip header guard defines
                elif '#include' in line:
                    continue  # Skip includes, we'll add them
            c_code_lines.append(line)

        c_code = '\n'.join(c_code_lines).strip()

        print("   Model converted successfully")
        print(f"   Generated C code size: {len(c_code)} bytes")
        print(f"   Model trees: {rf_model.n_estimators}, depth: {rf_model.max_depth}")
    except Exception as e:
        print(f"   ERROR: Conversion failed: {e}")
        print("   Trying alternative method...")
        try:
            # Try loadable method as fallback
            converter = emlearn.convert(rf_model, method='loadable')
            temp_c_file = os.path.join(SCRIPT_DIR, 'temp_temperature_model.h')
            converter.save(file=temp_c_file, name='temperature_model')
            with open(temp_c_file, 'r') as f:
                c_code = f.read()
            os.remove(temp_c_file)
            print("   Model converted with loadable method")
        except Exception as e2:
            print(f"   ERROR: Both methods failed: {e2}")
            sys.exit(1)

    # Generate header file with scaler parameters
    print("\n[3/4] Generating temperature_model.h header file...")

    header_content = f"""// !!! This file is generated by export_temperature_model_to_c.py !!!
// Temperature Prediction Model for Contiki-NG IoT Nodes
// Model: Random Forest Regressor ({rf_model.n_estimators} trees)
// Input: 48 temperature readings (24 hours at 30-min intervals)
// Output: Predicted temperature for next 30 minutes

#ifndef TEMPERATURE_MODEL_H
#define TEMPERATURE_MODEL_H

#include <stdint.h>
#include <math.h>

// Scaler parameters (MinMaxScaler)
#define TEMP_SCALER_MIN {scaler.data_min_[0]:.6f}f
#define TEMP_SCALER_MAX {scaler.data_max_[0]:.6f}f
#define TEMP_SCALER_RANGE {(scaler.data_max_[0] - scaler.data_min_[0]):.6f}f

// Model parameters
#define TEMP_MODEL_N_TREES {rf_model.n_estimators}
#define TEMP_MODEL_MAX_DEPTH {rf_model.max_depth}
#define TEMP_MODEL_SEQUENCE_LENGTH 48

// ============================================================================
// EMLEARN GENERATED MODEL CODE
// ============================================================================

{c_code}

// ============================================================================
// HELPER FUNCTIONS FOR IoT DEPLOYMENT
// ============================================================================

// Scale input value (temperature in Celsius) to [0, 1]
static inline float temperature_model_scale(float temp_celsius) {{
    return (temp_celsius - TEMP_SCALER_MIN) / TEMP_SCALER_RANGE;
}}

// Inverse scale output value [0, 1] to temperature in Celsius
static inline float temperature_model_inverse_scale(float scaled_value) {{
    return scaled_value * TEMP_SCALER_RANGE + TEMP_SCALER_MIN;
}}

// Predict next temperature given sequence of past 48 readings
// features: array of 48 scaled temperature values converted to int16_t
// returns: scaled prediction [0, 1] - use temperature_model_inverse_scale() to get Celsius
static inline float temperature_model_predict_scaled(const int16_t *features) {{
    return temperature_model_predict(features, TEMP_MODEL_SEQUENCE_LENGTH);
}}

// High-level prediction function
// past_temps: array of 48 temperature readings in Celsius
// returns: predicted temperature in Celsius
static inline float temperature_model_predict_celsius(const float *past_temps) {{
    int16_t scaled_features[TEMP_MODEL_SEQUENCE_LENGTH];

    // Scale input features and convert to int16_t (fixed-point representation)
    for (int i = 0; i < TEMP_MODEL_SEQUENCE_LENGTH; i++) {{
        float scaled = temperature_model_scale(past_temps[i]);
        // Convert float [0, 1] to int16_t [0, 32767] for model input
        scaled_features[i] = (int16_t)(scaled * 32767.0f);
    }}

    // Predict scaled value
    float scaled_prediction = temperature_model_predict_scaled(scaled_features);

    // Inverse scale to get Celsius
    return temperature_model_inverse_scale(scaled_prediction);
}}

#endif // TEMPERATURE_MODEL_H
"""

    # Create output directory in the airQualityGermany folder
    output_dir = os.path.join(SCRIPT_DIR, 'output')
    os.makedirs(output_dir, exist_ok=True)

    # Write header file to output subfolder
    output_path = os.path.join(output_dir, 'temperature_model.h')

    with open(output_path, 'w') as f:
        f.write(header_content)

    print(f"   Header file written: {output_path}")

    # Generate test/example file
    print("\n[4/4] Generating example usage file...")

    example_content = """/*
 * Example usage of temperature_model.h in Contiki-NG node
 *
 * This demonstrates how to predict future temperature using
 * the embedded Random Forest model with proper type conversions.
 */

#include "temperature_model.h"
#include <stdio.h>

// Example: Predict next temperature based on last 24 hours
static float predict_next_temperature(void) {{
    // Buffer to store last 48 temperature readings (24 hours at 30-min intervals)
    static float temperature_history[TEMP_MODEL_SEQUENCE_LENGTH];
    static int history_index = 0;
    static int history_filled = 0;

    // Current temperature reading (replace with actual sensor reading)
    float current_temp = (float)temperatureCelsius;

    // Add current reading to history
    temperature_history[history_index] = current_temp;
    history_index = (history_index + 1) % TEMP_MODEL_SEQUENCE_LENGTH;

    if (!history_filled && history_index == 0) {{
        history_filled = 1; // Buffer is now full
    }}

    // Only predict if we have enough history
    if (!history_filled) {{
        return current_temp; // Not enough data yet, return current temp
    }}

    // Rearrange buffer to have oldest reading first
    float ordered_history[TEMP_MODEL_SEQUENCE_LENGTH];
    for (int i = 0; i < TEMP_MODEL_SEQUENCE_LENGTH; i++) {{
        int idx = (history_index + i) % TEMP_MODEL_SEQUENCE_LENGTH;
        ordered_history[i] = temperature_history[idx];
    }}

    // Predict next temperature (handles conversion internally)
    float predicted_temp = temperature_model_predict_celsius(ordered_history);

    printf("Temperature Prediction:\\n");
    printf("  Current: %.2f°C\\n", current_temp);
    printf("  Predicted (next 30min): %.2f°C\\n", predicted_temp);
    printf("  Trend: %s%.2f°C\\n",
        predicted_temp > current_temp ? "+" : "",
        predicted_temp - current_temp);

    return predicted_temp;
}}

// Alternative: Manual prediction with direct int16_t conversion
static float predict_temperature_manual(const float *past_48_readings) {{
    int16_t scaled_features[TEMP_MODEL_SEQUENCE_LENGTH];

    // Manually scale and convert to int16_t
    for (int i = 0; i < TEMP_MODEL_SEQUENCE_LENGTH; i++) {{
        float scaled = (past_48_readings[i] - TEMP_SCALER_MIN) / TEMP_SCALER_RANGE;
        scaled_features[i] = (int16_t)(scaled * 32767.0f);
    }}

    // Call model directly
    float scaled_prediction = temperature_model_predict(scaled_features, TEMP_MODEL_SEQUENCE_LENGTH);

    // Convert back to Celsius
    return scaled_prediction * TEMP_SCALER_RANGE + TEMP_SCALER_MIN;
}}

// Simplified: Use the high-level helper function
static float predict_temperature_simple(const float *past_48_readings) {{
    // This function handles all conversions internally
    return temperature_model_predict_celsius(past_48_readings);
}}
"""

    # Write example file to output subfolder
    example_path = os.path.join(output_dir, 'temperature_prediction_example.c')
    with open(example_path, 'w') as f:
        f.write(example_content)

    print(f"   Example file written: {example_path}")

    # Print usage summary
    print("\n" + "="*70)
    print("SUCCESS: Temperature model exported to C!")
    print("="*70)
    print(f"""
Generated Files:
    - {output_path}
    - {example_path}

Model Info:
    - Trees: {rf_model.n_estimators}
    - Max Depth: {rf_model.max_depth}
    - Input: 48 temperature readings (24 hours)
    - Output: Predicted temperature for next 30 minutes
    - Temperature range: {scaler.data_min_[0]:.2f}°C to {scaler.data_max_[0]:.2f}°C

Usage in node1.c:
    1. Copy temperature_model.h to your Contiki-NG node directory
    2. Include the header: #include "temperature_model.h"
    3. Collect 48 temperature readings over 24 hours
    4. Call: float prediction = temperature_model_predict_celsius(readings);

Type Conversion Notes:
    - The ML model expects int16_t input (emlearn requirement)
    - Helper function temperature_model_predict_celsius() handles conversions:
      * Celsius → Scaled [0, 1] → int16_t [0, 32767] → Model → Celsius
    - Manual conversion: scaled_int16 = (int16_t)((temp - min) / range * 32767.0f)

Important Notes:
    - Model requires 48 historical readings (24 hours at 30-min intervals)
    - For 15-second intervals, you'll need to downsample or adjust
    - Consider memory constraints: RF models can be large
    - Current model size: ~{rf_model.n_estimators * rf_model.max_depth * 100} bytes (estimate)

Memory Optimization:
    If model is too large, retrain with fewer trees:
    - Current: {rf_model.n_estimators} trees
    - Try: less trees for embedded systems
    - Edit 2023_indoor_air_quality_dataset_germany.py line with n_estimators
""")

    print("="*70)

if __name__ == '__main__':
    # We now use SCRIPT_DIR for all paths, so no need to check current directory
    print(f"Running from directory: {SCRIPT_DIR}")

    export_rf_model_to_c()
